{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "optical-passing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# TensorFlow ≥2.0 is required<\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from keras.optimizers import RMSprop\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "#tf 1.x , keras > 2.2 , kerassurgeon \n",
    "#tf.compat.v1.disable_v2_behavior() #shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "informal-wings",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python ≥3.5 is required\n",
    "import sys\n",
    "assert sys.version_info >= (3, 5)\n",
    "\n",
    "# Scikit-Learn ≥0.20 is required\n",
    "import sklearn\n",
    "assert sklearn.__version__ >= \"0.20\"\n",
    "\n",
    "\n",
    "# TensorFlow ≥2.0 is required\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "assert tf.__version__ >= \"2.0\"\n",
    "\n",
    "# Common imports\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "# to make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "grand-metadata",
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "guided-algeria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 3, 0, 5], dtype=uint8)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "wanted-scheme",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,\n",
       "          0,   0,  13,  73,   0,   0,   1,   4,   0,   0,   0,   0,   1,\n",
       "          1,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   3,\n",
       "          0,  36, 136, 127,  62,  54,   0,   0,   0,   1,   3,   4,   0,\n",
       "          0,   3],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,\n",
       "          0, 102, 204, 176, 134, 144, 123,  23,   0,   0,   0,   0,  12,\n",
       "         10,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0, 155, 236, 207, 178, 107, 156, 161, 109,  64,  23,  77, 130,\n",
       "         72,  15],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   0,\n",
       "         69, 207, 223, 218, 216, 216, 163, 127, 121, 122, 146, 141,  88,\n",
       "        172,  66],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   1,   1,   0,\n",
       "        200, 232, 232, 233, 229, 223, 223, 215, 213, 164, 127, 123, 196,\n",
       "        229,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        183, 225, 216, 223, 228, 235, 227, 224, 222, 224, 221, 223, 245,\n",
       "        173,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "        193, 228, 218, 213, 198, 180, 212, 210, 211, 213, 223, 220, 243,\n",
       "        202,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   1,   3,   0,  12,\n",
       "        219, 220, 212, 218, 192, 169, 227, 208, 218, 224, 212, 226, 197,\n",
       "        209,  52],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   6,   0,  99,\n",
       "        244, 222, 220, 218, 203, 198, 221, 215, 213, 222, 220, 245, 119,\n",
       "        167,  56],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   4,   0,   0,  55,\n",
       "        236, 228, 230, 228, 240, 232, 213, 218, 223, 234, 217, 217, 209,\n",
       "         92,   0],\n",
       "       [  0,   0,   1,   4,   6,   7,   2,   0,   0,   0,   0,   0, 237,\n",
       "        226, 217, 223, 222, 219, 222, 221, 216, 223, 229, 215, 218, 255,\n",
       "         77,   0],\n",
       "       [  0,   3,   0,   0,   0,   0,   0,   0,   0,  62, 145, 204, 228,\n",
       "        207, 213, 221, 218, 208, 211, 218, 224, 223, 219, 215, 224, 244,\n",
       "        159,   0],\n",
       "       [  0,   0,   0,   0,  18,  44,  82, 107, 189, 228, 220, 222, 217,\n",
       "        226, 200, 205, 211, 230, 224, 234, 176, 188, 250, 248, 233, 238,\n",
       "        215,   0],\n",
       "       [  0,  57, 187, 208, 224, 221, 224, 208, 204, 214, 208, 209, 200,\n",
       "        159, 245, 193, 206, 223, 255, 255, 221, 234, 221, 211, 220, 232,\n",
       "        246,   0],\n",
       "       [  3, 202, 228, 224, 221, 211, 211, 214, 205, 205, 205, 220, 240,\n",
       "         80, 150, 255, 229, 221, 188, 154, 191, 210, 204, 209, 222, 228,\n",
       "        225,   0],\n",
       "       [ 98, 233, 198, 210, 222, 229, 229, 234, 249, 220, 194, 215, 217,\n",
       "        241,  65,  73, 106, 117, 168, 219, 221, 215, 217, 223, 223, 224,\n",
       "        229,  29],\n",
       "       [ 75, 204, 212, 204, 193, 205, 211, 225, 216, 185, 197, 206, 198,\n",
       "        213, 240, 195, 227, 245, 239, 223, 218, 212, 209, 222, 220, 221,\n",
       "        230,  67],\n",
       "       [ 48, 203, 183, 194, 213, 197, 185, 190, 194, 192, 202, 214, 219,\n",
       "        221, 220, 236, 225, 216, 199, 206, 186, 181, 177, 172, 181, 205,\n",
       "        206, 115],\n",
       "       [  0, 122, 219, 193, 179, 171, 183, 196, 204, 210, 213, 207, 211,\n",
       "        210, 200, 196, 194, 191, 195, 191, 198, 192, 176, 156, 167, 177,\n",
       "        210,  92],\n",
       "       [  0,   0,  74, 189, 212, 191, 175, 172, 175, 181, 185, 188, 189,\n",
       "        188, 193, 198, 204, 209, 210, 210, 211, 188, 188, 194, 192, 216,\n",
       "        170,   0],\n",
       "       [  2,   0,   0,   0,  66, 200, 222, 237, 239, 242, 246, 243, 244,\n",
       "        221, 220, 193, 191, 179, 182, 182, 181, 176, 166, 168,  99,  58,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,  40,  61,  44,  72,  41,  35,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0],\n",
       "       [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "          0,   0]], dtype=uint8)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full[0]#60 000 images de 28*28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "great-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "X_valid, X_train = X_train_full[:5000], X_train_full[5000:]\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "central-relationship",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_train_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "promotional-sheep",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9, 0, 0, ..., 5, 8, 3], dtype=uint8)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "preceding-hacker",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(len(X_train))\n",
    "valid_set = tf.data.Dataset.from_tensor_slices((X_valid, y_valid))\n",
    "test_set = tf.data.Dataset.from_tensor_slices((X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "possible-yahoo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ShuffleDataset shapes: ((28, 28), ()), types: (tf.uint8, tf.uint8)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-hours",
   "metadata": {},
   "source": [
    "# TFRECORD format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "vital-inflation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.train import BytesList, FloatList, Int64List\n",
    "from tensorflow.train import Feature, Features, Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "prescribed-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "BytesList = tf.train.BytesList\n",
    "FloatList = tf.train.FloatList\n",
    "Int64List = tf.train.Int64List\n",
    "Feature = tf.train.Feature\n",
    "Features = tf.train.Features\n",
    "Example = tf.train.Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "clean-bicycle",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "features {\n",
       "  feature {\n",
       "    key: \"image\"\n",
       "    value {\n",
       "      bytes_list {\n",
       "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\007\\007\\000\\000\\000\\000\\213\\262\\211UL3\\n\\010\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000\\000\\000\\000\\000\\021ZZ\\0019\\260\\256\\220\\225\\264\\203\\227\\205U\\020\\000\\001\\000\\000\\000\\000\\000\\001\\000\\000\\\\3j\\256ZE\\233Z\\000,Y|Y\\005\\034\\036\\025\\005\\000\\000\\000\\000\\000\\001\\001\\000%\\231\\245\\264\\000\\000!*,\\036\\001#\\214>\\000\\000\\000\\000\\000\\000\\000\\000\\001\\001\\001\\001\\000\\036E\\260\\000\\240\\276\\000\\000\\000\\000\\000\\000\\000\\001N\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000l9\\377\\000\\000\\335%\\000\\000\\000\\001\\016*JWJ7%))\\025\\003\\000\\027\\\\IA)j5\\375.\\\\\\225\\225w\\265\\265\\271\\315\\335\\322\\273\\353\\311\\300\\271\\256\\247\\325P\\020`bpnn\\\\is\\211^PC\\'\\023\\000\\000\\000\\000\\000\\307\\253\\264\\271\\271\\273\\265.\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\"\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  feature {\n",
       "    key: \"label\"\n",
       "    value {\n",
       "      int64_list {\n",
       "        value: 5\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for image, label in train_set.take(1):\n",
    "    image_ser = tf.io.serialize_tensor(image)\n",
    "    image_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "            \"image\": Feature(bytes_list=BytesList(value=[image_ser.numpy()])),\n",
    "            \"label\": Feature(int64_list=Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "        )\n",
    "image_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "starting-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.io.TFRecordWriter(\"protobufs/mes_images\") as f:\n",
    "    for image, label in train_set:\n",
    "        image_ser = tf.io.serialize_tensor(image)\n",
    "        image_example = Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "            \"image\": Feature(bytes_list=BytesList(value=[image_ser.numpy()])),\n",
    "            \"label\": Feature(int64_list=Int64List(value=[label]))\n",
    "            }\n",
    "        )\n",
    "        )\n",
    "        f.write(image_example.SerializeToString())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accepting-variance",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example(image, label):\n",
    "    image_data = tf.io.serialize_tensor(image)\n",
    "    #image_data = tf.io.encode_jpeg(image[..., np.newaxis])\n",
    "    return Example(\n",
    "        features=Features(\n",
    "            feature={\n",
    "                \"image\": Feature(bytes_list=BytesList(value=[image_data.numpy()])),\n",
    "                \"label\": Feature(int64_list=Int64List(value=[label])),\n",
    "            }))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "instructional-temperature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "features {\n",
      "  feature {\n",
      "    key: \"image\"\n",
      "    value {\n",
      "      bytes_list {\n",
      "        value: \"\\010\\004\\022\\010\\022\\002\\010\\034\\022\\002\\010\\034\\\"\\220\\006\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000\\000\\210\\250d\\000\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\003\\000\\016\\276sE\\262\\220\\000\\004\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\000\\000\\240+\\000\\000\\000\\225)\\000\\005\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\006\\000p~\\000\\005\\003\\000\\n\\233\\000\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\262\\000\\000\\000\\000\\000\\000\\220\\005\\000\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\001\\004\\004\\007\\000D\\321\\000\\000\\004\\004\\000\\000\\356G\\000\\004\\003\\002\\003\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000W\\325\\000\\000\\000\\000\\000\\000\\326r\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000DlbhQ\\251\\326XS]YZ<\\266\\252;UKM\\031\\000\\000\\000\\000\\000\\000\\000\\000\\251\\347\\354\\361\\366\\333\\345\\372\\365\\355\\355\\361\\371\\351\\350\\375\\367\\366\\346h\\000\\000\\000\\000\\000\\000\\000\\000j\\320\\331\\336\\336\\336\\345\\344\\340\\334\\334\\335\\340\\352\\352\\344\\345\\346\\323+\\000\\000\\000\\000\\000\\000\\000\\000j\\341\\334\\347\\341\\346\\344\\340\\345\\346\\344\\343\\344\\353\\350\\347\\350\\352\\353\\\"\\000\\000\\000\\000\\000\\000\\000\\000g\\336\\334\\350\\340\\334\\334\\336\\343\\341\\345\\347\\346\\347\\347\\350\\346\\352\\345\\\"\\000\\000\\000\\000\\000\\000\\000\\000^\\333\\326\\344\\334\\333\\333\\336\\343\\344\\347\\352\\350\\350\\351\\353\\346\\351\\334\\035\\000\\000\\000\\000\\000\\000\\000\\000Q\\335\\316\\346\\334\\333\\335\\341\\344\\347\\352\\353\\351\\350\\350\\353\\350\\351\\331\\033\\000\\000\\000\\000\\000\\000\\000\\000C\\335\\312\\346\\331\\327\\332\\336\\341\\344\\350\\347\\345\\345\\345\\347\\350\\351\\332\\027\\000\\000\\000\\000\\000\\000\\000\\0008\\361\\311\\364\\347\\352\\356\\362\\364\\373\\371\\372\\373\\372\\372\\377\\355\\362\\366\\036\\000\\000\\000\\000\\000\\000\\000\\000\\006\\306\\254\\241\\262\\270\\260\\275\\305\\265\\305\\312\\271\\305\\300\\270\\301\\273\\306!\\000\\000\\000\\000\\000\\000\\000\\000\\000\\301\\270\\216\\263\\266\\221\\260\\263\\221\\266\\266\\226\\264\\243\\215\\256\\241\\256\\025\\000\\000\\000\\000\\000\\000\\000\\000\\000\\236\\221t\\227\\235x\\234\\236{\\227\\230\\200\\240\\233\\213\\241\\224\\245\\001\\000\\000\\000\\000\\000\\000\\000\\000\\000\\261\\263\\236\\301\\304\\247\\312\\320\\263\\314\\306\\253\\314\\307\\265\\325\\261\\320\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\236\\251t\\236\\236\\203\\245\\247\\214\\250\\250\\215\\255\\242\\224\\246\\222\\235\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\234\\262\\234\\303\\302\\244\\306\\306\\247\\303\\305\\245\\305\\274\\253\\301\\242\\234\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\215\\251\\202\\245\\251\\202\\234\\241\\202\\242\\252\\207\\245\\245\\207\\251\\246\\220\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000y\\266\\225\\301\\311\\256\\311\\315\\253\\312\\311\\255\\276\\310\\250\\255\\255\\210\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000[\\266\\223\\253\\260\\225\\256\\263\\224\\255\\263\\241\\253\\274\\244\\261\\266t\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\035\\261\\231\\272\\301\\251\\275\\312\\264\\275\\311\\266\\274\\306\\254\\260\\2642\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\265\\243\\244\\263\\227\\250\\274\\241\\256\\302\\252\\265\\324\\260\\254\\331\\016\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000\\000I\\241\\244\\264\\247\\246\\264\\245\\247\\266\\243\\221\\227\\203\\216e\\000\\000\\000\\000\\000\\000\"\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  feature {\n",
      "    key: \"label\"\n",
      "    value {\n",
      "      int64_list {\n",
      "        value: 8\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_set.take(1):\n",
    "    print(create_example(image, label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-arrest",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_descriptions = {\n",
    "        \"image\": tf.io.FixedLenFeature([], tf.string, default_value=\"\"),\n",
    "        \"label\": tf.io.FixedLenFeature([], tf.int64, default_value=-1)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "separate-validation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 <ShuffleDataset shapes: (), types: tf.string>\n",
      "Tensor(\"ParseTensor:0\", dtype=uint8)\n",
      "Tensor(\"image:0\", shape=(28, 28), dtype=uint8)\n",
      "Tensor(\"ParseExample/ParseExampleV2:1\", shape=(), dtype=int64)\n",
      "1 <MapDataset shapes: ((28, 28), ()), types: (tf.uint8, tf.int64)>\n",
      "2 <BatchDataset shapes: ((None, 28, 28), (None,)), types: (tf.uint8, tf.int64)>\n",
      "3 <PrefetchDataset shapes: ((None, 28, 28), (None,)), types: (tf.uint8, tf.int64)>\n"
     ]
    }
   ],
   "source": [
    "from keras import Input\n",
    "\n",
    "def parse_examples(serialized_examples):\n",
    "    examples = tf.io.parse_example(serialized_examples, feature_descriptions)\n",
    "    #targets = examples.pop(\"label\") # separate the targets\n",
    "    image = tf.io.parse_tensor(examples[\"image\"], out_type=tf.uint8)\n",
    "    print(image)\n",
    "    image = tf.reshape(image, shape=[28, 28], name=\"image\")\n",
    "    print(image)\n",
    "    print(examples[\"label\"])\n",
    "    examples[\"image\"] = image\n",
    "    return image, examples[\"label\"]  #examples #et non return image, examples[\"label\"] sinon on ne peut pas accéder aux noms des tensors...\n",
    "\n",
    "batch_size = 32\n",
    "train_set = tf.data.TFRecordDataset([\"protobufs/mes_images\"])\n",
    "train_set = train_set.shuffle(60000)\n",
    "print(\"0\", train_set)\n",
    "train_set = train_set.map(parse_examples)\n",
    "print(\"1\", train_set)\n",
    "train_set = train_set.batch(batch_size)\n",
    "print(\"2\", train_set)\n",
    "train_set = train_set.prefetch(1)\n",
    "print(\"3\", train_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "stainless-devices",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PrefetchDataset shapes: ((None, 28, 28), (None,)), types: (tf.uint8, tf.int64)>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "wooden-correlation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 1  0  0 ... 61 29  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]], shape=(32, 28, 28), dtype=uint8)\n",
      "tf.Tensor([9 6 4 4 7 2 6 6 9 5 9 9 3 1 2 6 4 7 0 2 4 1 4 8 0 3 1 8 7 5 0 1], shape=(32,), dtype=int64)\n",
      "tf.Tensor(\n",
      "[[[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  1  0 ... 40  0  1]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]], shape=(32, 28, 28), dtype=uint8)\n",
      "tf.Tensor([5 8 8 5 6 1 7 8 9 2 1 1 5 6 3 5 2 9 0 2 1 5 3 3 8 8 0 8 4 9 5 7], shape=(32,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for image, label in train_set.take(2):\n",
    "    print(image)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "covered-suggestion",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "working-rocket",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 3.0160 - accuracy: 0.6623 - val_loss: 0.8262 - val_accuracy: 0.6810\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.7191 - accuracy: 0.7126 - val_loss: 0.7908 - val_accuracy: 0.7708\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6271 - accuracy: 0.7735 - val_loss: 0.5726 - val_accuracy: 0.8100\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.5590 - accuracy: 0.8023 - val_loss: 0.5679 - val_accuracy: 0.8070\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 0.5167 - accuracy: 0.8199 - val_loss: 0.5169 - val_accuracy: 0.8292\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe5b80dbc40>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28, 1]),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])\n",
    "model.fit(train_set, validation_data=(X_valid, y_valid),  epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "minute-eating",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 100)               78500     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                1010      \n",
      "=================================================================\n",
      "Total params: 79,510\n",
      "Trainable params: 79,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Model\n",
    "\n",
    "input_image = Input(shape=(28,28,), dtype='uint8')\n",
    "x= keras.layers.Flatten()(input_image)\n",
    "x =keras.layers.Dense(100, activation=\"relu\")(x)\n",
    "output = keras.layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "model = Model(input_image, output)\n",
    "model.summary()\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"nadam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "naval-europe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1719/1719 [==============================] - 3s 1ms/step - loss: 3.6214 - accuracy: 0.7065 - val_loss: 0.9087 - val_accuracy: 0.7380\n",
      "Epoch 2/5\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.6986 - accuracy: 0.7554 - val_loss: 0.6517 - val_accuracy: 0.7780\n",
      "Epoch 3/5\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5866 - accuracy: 0.7835 - val_loss: 0.5729 - val_accuracy: 0.8026\n",
      "Epoch 4/5\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.5222 - accuracy: 0.8103 - val_loss: 0.5286 - val_accuracy: 0.8236\n",
      "Epoch 5/5\n",
      "1719/1719 [==============================] - 2s 1ms/step - loss: 0.4882 - accuracy: 0.8289 - val_loss: 0.4993 - val_accuracy: 0.8444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fe633947cd0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_set, validation_data=(X_valid, y_valid), epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naval-cooking",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
